{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=200 height=200 align=left class=\"saturate\" >\n",
    "\n",
    "<br>\n",
    "<font face=\"Times New Roman\">\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    "    Introduction to Machine Learning <br>\n",
    "<font color=2565AE size=5>\n",
    "    Computer Engineering Department <br>\n",
    "    Fall 2022<br>\n",
    "<font color=3C99D size=5>\n",
    "    Homework 3: Practical - Neural Network <br>\n",
    "<font color=696880 size=4>\n",
    "    Javad Hezareh \n",
    "    \n",
    "    \n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Name : Mohammad Jamshidi\n",
    "### Student Number : 98100718\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "In this assignment our goal is to develop a framework for simple neural networks, multi layer perceptrons. We are going to use only `numpy` and no other packages to build our own classes and network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D_a46D-Gz3Rj"
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "#  Do Not Add any other packages  #\n",
    "###################################\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import copy\n",
    "from utils import *\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Modules implementation (65 Points)\n",
    "We are going to implement required modules for a neural net. Each of this modules must implement the neccessery functions, `_forward` and `backward`. In the following parts, we will implement `LinearLayer`, `ReLU` and `SoftMax` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Layer (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "moGEgpV999cl"
   },
   "outputs": [],
   "source": [
    "class LinearLayer(Module):\n",
    "    \"\"\"\n",
    "    A linear layer module which calculate (Wx + b).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, initializer, reg, alpha):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - dim_in: input dimension,\n",
    "            - dim_out: output dimension,\n",
    "            - initializer: a function which get (dim_in, dim_out) and initialize\n",
    "                a [dim_in x dim_out] matrix,\n",
    "            - reg: L2-regularization flag\n",
    "            - alpha: L2-regularization coefficient\n",
    "        \"\"\"\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        #shoul i add these here?\n",
    "        self.reg = reg\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.params = {\n",
    "            #########################################\n",
    "            ##          Initialize parameters      ##\n",
    "            ##              Your Code              ##\n",
    "            #########################################\n",
    "            'W': initializer(dim_out, dim_in),\n",
    "            'b': np.zeros(dim_out)\n",
    "        }\n",
    "        self.grads = dict()\n",
    "        self.cache = dict()\n",
    "\n",
    "    def _forward(self, x):\n",
    "        \"\"\"\n",
    "        linear forward function, calculate Wx+b for a batch of data\n",
    "\n",
    "        Args:\n",
    "            x : a batch of data\n",
    "\n",
    "        Note:\n",
    "            you need to store some values in cache to be able to\n",
    "            calculate backward path.\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        # print(self.params['W'].shape)\n",
    "        # print(x.shape)\n",
    "        # print(self.params['b'].shape)\n",
    "        # b_prime = np.array([self.params['b'] for _ in range(x.shape[0])])\n",
    "        # print(b_prime.shape)\n",
    "        self.cache['x'] = x\n",
    "        # y = np.dot(x, self.params['W']) + np.array([self.params['b'] for _ in range(x.shape[0])])\n",
    "        y = np.dot(x, self.params['W']) + self.params['b']\n",
    "        return y\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        \"\"\"\n",
    "        get upstream gradient and returns downstream gradient\n",
    "\n",
    "        Args:\n",
    "            upstream : upstream gradient of loss w.r.t module output\n",
    "\n",
    "        Note:\n",
    "            you need to calculate gradient of loss w.r.t module input\n",
    "            and parameters and store them in grads.\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        grad_b = np.sum(upstream, axis=0)\n",
    "        grad_w = np.dot(self.cache['x'].T, upstream)\n",
    "        grad_x = np.dot(upstream, self.params['W'].T)\n",
    "        grad_reg = 2 * self.alpha * self.params['W']\n",
    "\n",
    "        self.grads = {\n",
    "            'W': grad_w,\n",
    "            'b': grad_b,\n",
    "            'x': grad_x,\n",
    "            'reg': grad_reg\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t output:\n",
      "[[0.82955535 0.21114721 0.29056745 0.76196253 0.48381119 0.26313752\n",
      "  0.17058744 0.68924104 0.14521547 0.36108282]\n",
      " [0.17969557 0.61131998 0.90547691 0.09465278 0.04351438 0.82898441\n",
      "  0.53347611 0.25746229 0.83709683 0.91589097]\n",
      " [0.95338569 0.53886974 0.68147718 0.14543317 0.36605854 0.25725115\n",
      "  0.64065521 0.99467217 0.84735271 0.6116583 ]\n",
      " [0.6585274  0.09264464 0.55180917 0.84839143 0.74920247 0.66146217\n",
      "  0.48228694 0.96546312 0.48159736 0.23890255]\n",
      " [0.62490249 0.71391215 0.39654666 0.08349731 0.25206548 0.14235356\n",
      "  0.84943899 0.30579129 0.18025764 0.72632517]\n",
      " [0.07006783 0.77688796 0.90754448 0.97929669 0.15398316 0.39885902\n",
      "  0.85437948 0.42579468 0.34963434 0.91473698]\n",
      " [0.92071169 0.33599422 0.31872931 0.63541889 0.58434891 0.17077758\n",
      "  0.11395208 0.59733395 0.64102327 0.2976854 ]\n",
      " [0.36232214 0.94163488 0.10551311 0.04220206 0.55321597 0.03134844\n",
      "  0.82945429 0.71557495 0.4421455  0.89943093]\n",
      " [0.15358404 0.01888662 0.93738637 0.70363497 0.25615749 0.52405876\n",
      "  0.07851757 0.82801702 0.82128155 0.6839097 ]\n",
      " [0.43418109 0.88647521 0.26329912 0.97095353 0.84626562 0.99043451\n",
      "  0.17365826 0.97193332 0.63421181 0.30386288]]\n",
      "Gradient of loss w.r.t input:\n",
      "[[ 2.54242696  1.45435661 -0.82022594  2.9063582  -2.17595982]\n",
      " [ 2.98865636  2.51049642 -0.37295633  3.35961307  0.48113702]\n",
      " [ 3.43164927  2.80023473 -2.96742325  3.90327717 -0.44075442]\n",
      " [ 3.20487055  1.90537264 -2.01530479  2.4314343  -1.91123847]\n",
      " [ 2.31601301  2.21729462 -3.20963396  4.50296474  0.16588643]\n",
      " [ 2.92477652  1.98756418 -0.45081362  2.8037088   0.64059953]\n",
      " [ 3.12306514  1.32619278 -1.29240403  2.39416553 -1.1895336 ]\n",
      " [ 2.09392444  2.05517408 -3.8628501   4.60660956  0.76814621]\n",
      " [ 2.46823865  1.46641717  1.25895091  1.36466138 -1.14954286]\n",
      " [ 3.70687368  2.3236876  -0.57120104  2.34628371 -0.9321634 ]]\n",
      "Gradient of loss w.r.t W:\n",
      "[[-0.07413355  1.38987484  0.00630421 -1.30000989 -0.69867041 -0.00371523\n",
      "   0.77006194 -1.75200249  0.08158199  1.07245762]\n",
      " [ 0.8180963   1.75706974  1.9269733   2.46883827  0.79617667  0.94336928\n",
      "   1.73546778  2.33289953  0.8489505   2.17299004]\n",
      " [-1.73793658  1.70546404  0.60665643 -2.94957165 -1.6757166   0.06701201\n",
      "   1.20094719 -2.01195638  0.76333291  2.54049985]\n",
      " [-0.45139637  1.23489814 -0.28003495 -1.16933264 -0.78370106 -1.82406556\n",
      "   0.88269164 -0.91585754 -0.2555532   1.16759882]\n",
      " [ 3.23746074  0.52410581  1.57116444  2.30895259  2.21580388  1.88801468\n",
      "   1.55779322  3.49752565  1.17878441  0.44437057]]\n",
      "Gradient of loss w.r.t b:\n",
      "[5.18693329 5.12777261 5.35834976 5.26544336 4.28862321 4.26866713\n",
      " 4.72640636 6.75128384 5.37981648 5.9534857 ]\n",
      "Relative error of delta-loss (for linear unit):\n",
      "9.237713583124246e-06\n",
      "Relative error of delta-loss (for regularization):\n",
      "1.1356366649269335e-05\n"
     ]
    }
   ],
   "source": [
    "# sanity check, output must be from o(e-5)\n",
    "initializer = lambda x, y: np.random.normal(size=(y, x))\n",
    "linear = LinearLayer(5, 10, initializer, reg=True, alpha=1)\n",
    "check_gradient_linear(linear, h=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU Layer (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RSsLumM29_eT"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cache = dict()\n",
    "        self.grads = dict()\n",
    "\n",
    "    def _forward(self, x):\n",
    "        \"\"\"\n",
    "        applies relu function on x\n",
    "\n",
    "        Args:\n",
    "            x : a batch of data\n",
    "\n",
    "        Returns:\n",
    "            y : relu of input\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        self.cache['x'] = x\n",
    "        return x * (x>0)\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        \"\"\"\n",
    "        calculate and store gradient of loss w.r.t module input\n",
    "\n",
    "        Args:\n",
    "            upstream : gradient of loss w.r.t modele output\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        x = self.cache['x']\n",
    "        grad_x = upstream * (x > 0)\n",
    "        \n",
    "        self.grads['x'] = grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t output:\n",
      "[[0.0584373  0.88325126 0.26846461 0.89464548 0.50320908]\n",
      " [0.09177401 0.38804983 0.72265477 0.53096799 0.3455338 ]\n",
      " [0.24378035 0.21950653 0.93640053 0.87820106 0.33644566]\n",
      " [0.87531923 0.36254974 0.54245228 0.95475059 0.96677731]\n",
      " [0.52121319 0.0189511  0.32733035 0.67793154 0.20010495]\n",
      " [0.7436883  0.46844046 0.26309046 0.4595906  0.83244012]\n",
      " [0.62944357 0.72287473 0.73348264 0.21539666 0.77413853]\n",
      " [0.20357441 0.51746258 0.75884962 0.6687436  0.51406511]\n",
      " [0.71992879 0.77988594 0.72842911 0.12522849 0.8795159 ]\n",
      " [0.64932805 0.24568954 0.25430494 0.42654584 0.25253977]]\n",
      "Gradient of loss w.r.t input:\n",
      "[[0.         0.88325126 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.93640053 0.         0.33644566]\n",
      " [0.87531923 0.36254974 0.         0.         0.96677731]\n",
      " [0.52121319 0.0189511  0.         0.67793154 0.20010495]\n",
      " [0.         0.46844046 0.26309046 0.         0.        ]\n",
      " [0.62944357 0.         0.         0.21539666 0.        ]\n",
      " [0.20357441 0.         0.75884962 0.         0.51406511]\n",
      " [0.         0.         0.72842911 0.         0.        ]\n",
      " [0.64932805 0.24568954 0.25430494 0.42654584 0.25253977]]\n",
      "Relative error of delta-loss:\n",
      "2.484727903125079e-14\n"
     ]
    }
   ],
   "source": [
    "# sanity check - output must be from o(e-8)\n",
    "relu = ReLU()\n",
    "check_gradient_relu(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftMax Layer (15 Points)\n",
    "\n",
    "We could have a layer that calculate softmax for us. In other word, for input $x\\in\\mathcal{R}^N$ it would return $y\\in\\mathcal{R}^n$ where $y_i = \\frac{e^{x_i}}{\\sum e^{x_i}}$. But this method is not numerical stable because $e^{x_i}$ in this formulation can get very large easly and return `nan`. Instead of that we will implement a logarithmic version of softmax which instead of calculating $\\frac{e^{x_i}}{\\sum e^{x_i}}$, we will calculate $\\log\\left(\\frac{e^{x_i}}{\\sum e^{x_i}}\\right) = x_i - \\log\\sum e^{x_i}$. In order to calculate second term you can use `np.logaddexp` but this function only works on two input. For more than two input, fill in the following function to be able to calculate log sum exp of an array of shape (b,n). `axis=1` means sum over columns and `axis=0` sum over rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(array, axis=1):\n",
    "    \"\"\"\n",
    "    calculate log(sum(exp(array))) using np.logaddexp\n",
    "\n",
    "    Args:\n",
    "        array : input array\n",
    "        axis : reduce axis, 1 means columns and 0 means rows\n",
    "    \"\"\"\n",
    "    if axis==0:\n",
    "        array = array.T\n",
    "        \n",
    "    result = array[:, 0]\n",
    "    assert len(array) >= 2\n",
    "    #########################################\n",
    "    ##              Your Code              ##\n",
    "    #########################################\n",
    "    for i in range(1, array.shape[1]):\n",
    "        result = np.logaddexp(result, array[:, i])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "R4v5_UBB-BCK"
   },
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "        self.cache = dict()\n",
    "        self.grads = dict()\n",
    "    \n",
    "    def _forward(self, x):\n",
    "        \"\"\"\n",
    "        get x and calculate softmax of that.\n",
    "        Args:\n",
    "            x : batch of data with shape (b,m)\n",
    "        Returns:\n",
    "            y : log softmax of x with shape (b,m)\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        y = x - logsumexp(x, axis=1)[:,None]\n",
    "        self.cache['y'] = y\n",
    "        self.cache['x'] = x\n",
    "        return y\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        \"\"\"\n",
    "        calculate gradient of loss w.r.t module input and save that in grads.\n",
    "\n",
    "        Args:\n",
    "            upstream : gradient of loss w.r.t module output with sahpe (b,m)\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        grad_y_ = np.diag(np.sum(upstream, axis=1))\n",
    "        grad_x = upstream - np.dot(grad_y_, np.exp(self.cache['y']))\n",
    "        self.grads['x'] = grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t output:\n",
      "[[0.1630331  0.13488387 0.64710355 0.267412   0.38909318]\n",
      " [0.17558247 0.28087961 0.28324271 0.94903849 0.49365313]\n",
      " [0.66110396 0.48183119 0.81942941 0.72267524 0.05972474]\n",
      " [0.23687506 0.58419898 0.63517021 0.34454597 0.22345524]\n",
      " [0.88441083 0.96840966 0.22989379 0.11408372 0.45871137]\n",
      " [0.00832097 0.74025973 0.43923602 0.0739073  0.96269913]\n",
      " [0.17190485 0.1383921  0.6479173  0.88009471 0.35911365]\n",
      " [0.42453736 0.08015594 0.24649505 0.0371008  0.95956135]\n",
      " [0.14149331 0.25348514 0.93081829 0.36581357 0.52428419]\n",
      " [0.23683714 0.9252707  0.18926396 0.4527474  0.47814843]]\n",
      "Gradient of loss w.r.t input:\n",
      "[[-0.25543381 -0.14106689  0.41398    -0.12588516  0.10840584]\n",
      " [-0.20376161 -0.15868754 -0.08018071  0.43744502  0.00518484]\n",
      " [ 0.11956483  0.16874444  0.31243645 -0.01058303 -0.59016269]\n",
      " [-0.09121993  0.20407432  0.31101343  0.00085981 -0.42472763]\n",
      " [ 0.28037124  0.35101985 -0.11664377 -0.4851247  -0.02962261]\n",
      " [-0.29306479  0.21679663 -0.12605471 -0.46330875  0.66563162]\n",
      " [-0.403257   -0.28535296  0.22465089  0.32906213  0.13489695]\n",
      " [ 0.02977464 -0.26441952 -0.26649578 -0.19206979  0.69321045]\n",
      " [-0.37287508 -0.16202289  0.67216183 -0.13421601 -0.00304785]\n",
      " [-0.1331356   0.33035269 -0.14465495 -0.06388172  0.01131958]]\n",
      "Relative error of delta-loss:\n",
      "1.3117848220960223e-07\n"
     ]
    }
   ],
   "source": [
    "# sanity check, output must be from o(e-7)\n",
    "sm = LogSoftMax()\n",
    "check_gradient_softmax(sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (10 Points)\n",
    "We need a model class which gathers our layers togather and performs forward and backward on all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(Module):\n",
    "    \"\"\"\n",
    "    A multilayer neural network model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layers : list of model layers\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def _forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform forward on x\n",
    "\n",
    "        Args:\n",
    "            x : a batch of data\n",
    "\n",
    "        Returns:\n",
    "            o : model output\n",
    "        \"\"\"\n",
    "        \n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        result = x\n",
    "        for layer in self.layers:\n",
    "            result = layer._forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        \"\"\"\n",
    "        Perform backward path on whole model\n",
    "\n",
    "        Args:\n",
    "            upstream : gradient of loss w.r.t model output\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        result = upstream\n",
    "        for layer in self.layers:\n",
    "            layer.backward(upstream)\n",
    "            result = layer.grads['x']\n",
    "        return result\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            parametric_layers : all layers of model which have parameter\n",
    "        \"\"\"\n",
    "        dict = {}\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            if layer.hasAttribute('params'):\n",
    "                dict[f'{str(type(layer).__name)}_{n}'] = layer.params\n",
    "                pass\n",
    "            pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions (10 Points)\n",
    "We need to implement loss functions to be able to train our network. We will implement CrossEntropy loss function. But notice that we have implemented `LogSoftMax` in logarithmic way so input of the following class will be logarithm of probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HIBN2fmBpYGG"
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    def __init__(self, mean=False):\n",
    "        self.mean = mean\n",
    "        self.cache = dict()\n",
    "        self.grads = dict()\n",
    "\n",
    "    def _forward(self, logprobs, targets):\n",
    "        \"\"\"\n",
    "        Calculate cross entropy of inputs.\n",
    "\n",
    "        Args:\n",
    "            probs : matrix of probabilities with shape (b,n)\n",
    "            targets : list of samples classes with shape (b,)\n",
    "\n",
    "        Returns:\n",
    "            y : cross entropy loss\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        batch_size = len(targets)\n",
    "        one_hot_encoding = np.eye(logprobs.shape[1])[targets]\n",
    "\n",
    "        self.cache['one_hot'] = one_hot_encoding\n",
    "        self.cache['size'] = batch_size\n",
    "        y = -(1 / batch_size) * np.sum(one_hot_encoding * logprobs)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        \"\"\"\n",
    "        Calculate gradient of loss w.r.t module input and save them in grads.\n",
    "\n",
    "        Args:\n",
    "            upstream : gradient of loss w.r.t module output (loss)\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        #actualy we know that here upstream is 1.\n",
    "        grad_scores = -(1 / self.cache['size']) * self.cache['one_hot'] * upstream\n",
    "        \n",
    "        self.grads['x'] = grad_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t output:\n",
      "1\n",
      "Gradient of loss w.r.t input:\n",
      "[[-0.  -0.1 -0.  -0.  -0. ]\n",
      " [-0.  -0.  -0.  -0.1 -0. ]\n",
      " [-0.  -0.  -0.  -0.  -0.1]\n",
      " [-0.1 -0.  -0.  -0.  -0. ]\n",
      " [-0.1 -0.  -0.  -0.  -0. ]\n",
      " [-0.  -0.1 -0.  -0.  -0. ]\n",
      " [-0.  -0.  -0.  -0.  -0.1]\n",
      " [-0.  -0.1 -0.  -0.  -0. ]\n",
      " [-0.  -0.1 -0.  -0.  -0. ]\n",
      " [-0.  -0.  -0.  -0.  -0.1]]\n",
      "Relative error of delta-loss:\n",
      "3.327145416815998e-13\n"
     ]
    }
   ],
   "source": [
    "# check gradient, output must be from o(e-10)\n",
    "ce = CrossEntropyLoss()\n",
    "check_gradient_ce(ce, h=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization (15 Points)\n",
    "\n",
    "Now that we have our network and loss function, we need to update model paremeters. We can do so by using `Optimizer` class that perform updating rule on model parameters. You need to implement `sgd` and `momentum` strategy for this optimizer. Becarefull to consider regularization update for linear units that require regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pDROgraatB0p"
   },
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"\n",
    "    Our main optimization class.\n",
    "    \n",
    "    You can add arguments to _sgd and _momentum function if you need to do so, and\n",
    "    pass this arguments to step function when using optimizer. Don't change __init__\n",
    "    or step function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, strategy, lr):\n",
    "        \"\"\"\n",
    "        save layers here in order to update their parameters later.\n",
    "\n",
    "        Args:\n",
    "            layers : model layers (those that we want to update their parameters)\n",
    "            strategy : optimization strategy\n",
    "            lr : learning rate\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.strategy = strategy\n",
    "        self.lr = lr\n",
    "        self.strategies = {\n",
    "            'sgd': self._sgd,\n",
    "            'momentum': self._momentum,\n",
    "        }\n",
    "\n",
    "    def step(self, *args):\n",
    "        \"\"\"\n",
    "        Perform updating strategy on all layers paramters.\n",
    "        \"\"\"\n",
    "        self.strategies[self.strategy](*args)\n",
    "        \n",
    "    def _sgd(self):\n",
    "        \"\"\"\n",
    "        Perform sgd update on all parameters of layers\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        for layer in self.layers:\n",
    "            if type(layer) == LinearLayer:\n",
    "                layer.params['W'] -= (layer.grads['W'] + layer.grads['reg']) * self.lr\n",
    "                layer.params['b'] -= (layer.grads['b']) * self.lr\n",
    "    \n",
    "    def _momentum(self):\n",
    "        \"\"\"\n",
    "        Perform momentum update on all parameters of layers\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        ##              Your Code              ##\n",
    "        #########################################\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: CIFAR-10 Classification (35 Points)\n",
    "\n",
    "Now that we can build a neural network we want to solve CIFAR-10 classification problem. This dataset consists of 60000 $32 \\times 32$ coloured images in 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "##      Run this cell to download dataset      ##\n",
    "##         the dataset is about 150 MB         ##\n",
    "#################################################\n",
    "!./cifar10_downloader.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cifar-10-batches-py/data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Academia\\BS\\7\\Machine Learning\\hw\\ML-practical-homeworks-fall-2022\\hw3\\Neural Network\\NeuralNet.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Academia/BS/7/Machine%20Learning/hw/ML-practical-homeworks-fall-2022/hw3/Neural%20Network/NeuralNet.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#############################################\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Academia/BS/7/Machine%20Learning/hw/ML-practical-homeworks-fall-2022/hw3/Neural%20Network/NeuralNet.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m##      Run this cell to load dataset      ##\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Academia/BS/7/Machine%20Learning/hw/ML-practical-homeworks-fall-2022/hw3/Neural%20Network/NeuralNet.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#############################################\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Academia/BS/7/Machine%20Learning/hw/ML-practical-homeworks-fall-2022/hw3/Neural%20Network/NeuralNet.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m load_dataset(train_num\u001b[39m=\u001b[39;49m\u001b[39m4000\u001b[39;49m, test_num\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Academia/BS/7/Machine%20Learning/hw/ML-practical-homeworks-fall-2022/hw3/Neural%20Network/NeuralNet.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Academia/BS/7/Machine%20Learning/hw/ML-practical-homeworks-fall-2022/hw3/Neural%20Network/NeuralNet.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdata[k]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Academia\\BS\\7\\Machine Learning\\hw\\ML-practical-homeworks-fall-2022\\hw3\\Neural Network\\utils.py:134\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(train_num, test_num)\u001b[0m\n\u001b[0;32m    132\u001b[0m Y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[0;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath\u001b[39m}\u001b[39;49;00m\u001b[39m/data_batch_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    135\u001b[0m         data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    136\u001b[0m         x \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cifar-10-batches-py/data_batch_1'"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "##      Run this cell to load dataset      ##\n",
    "#############################################\n",
    "data = load_dataset(train_num=4000, test_num=1000)\n",
    "\n",
    "for k in data.keys():\n",
    "    print(f'{k}: {data[k].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "##      Split train set to train/val        ##\n",
    "################[Your Code]###################\n",
    "\n",
    "data = load_dataset(train_num=4000, test_num=1000)\n",
    "\n",
    "\n",
    "##############################################\n",
    "for k in data.keys():\n",
    "    print(f'{k}: {data[k].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "##      Visualize 5 samples from each class     ##\n",
    "##################[Your Code]#####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##             Normalize and flatten X             ##\n",
    "####################[Your Code]######################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "for k in data.keys():\n",
    "    print(f'{k}: {data[k].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Model (25 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Sampler\n",
    "We need to sample bathces from our dataset to train model. Complete the following class to have a random sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSampler(object):\n",
    "    def __init__(self, batch_size, dataset, type):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size : sampler batch size\n",
    "            dataset : dataset we want to get batch from that\n",
    "            type : one of {'train', 'test', 'val'}\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.x_key = f'X_{type}'\n",
    "        self.y_key = f'Y_{type}'\n",
    "        self.indices = None\n",
    "        self.num_batches = None\n",
    "        ################################################################\n",
    "        ##       Build batches indices and store them in indices      ##\n",
    "        ##          Also store number of batches in num_batches       ##\n",
    "        ##                          Your Code                         ##\n",
    "        ################################################################\n",
    "\n",
    "    def __len__(self):\n",
    "        assert type(self.num_batches) == int\n",
    "        return self.num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        This function call when we iterate an object of this class and\n",
    "        yields one batch on each call.\n",
    "\n",
    "        Yields:\n",
    "            (x, y) : tuple of bathces of x and y\n",
    "        \"\"\"\n",
    "        for idx in self.indices:\n",
    "            x = self.dataset[self.x_key][idx]\n",
    "            y = self.dataset[self.y_key][idx]\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the following functions to update a confusion matrix and calculate f1 score for a confusion matrix. For multi class f1 score read [here](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_confusion_matrix(conf_matrix, preds, reals):\n",
    "    \"\"\"\n",
    "    Updates confusion matrix\n",
    "\n",
    "    Args:\n",
    "        conf_matrix : input confusion matrix\n",
    "        preds : array of predicted labels\n",
    "        reals : array of real labels\n",
    "\n",
    "    Returns:\n",
    "        conf_matrix : updated confusion matrix\n",
    "    \"\"\"\n",
    "    #################################\n",
    "    ##          Your Code          ##\n",
    "    #################################\n",
    "    return conf_matrix\n",
    "\n",
    "\n",
    "def f1_score(confusion_matrix):\n",
    "    \"\"\"\n",
    "    calculate macro f1 score from given confusion matrix\n",
    "\n",
    "    Args:\n",
    "        confusion_matrix : given confusion matrix\n",
    "        \n",
    "    Returns:\n",
    "        f1 : macro f1 score\n",
    "    \"\"\"\n",
    "    #################################\n",
    "    ##          Your Code          ##\n",
    "    #################################\n",
    "    f1_score = None\n",
    "    \n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "Define an MLP model to solve classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "##             Define your model             ##\n",
    "##     use a good initializer for layers     ##\n",
    "###############################################\n",
    "\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "##          Hyper parameters           ##\n",
    "#########################################\n",
    "\n",
    "n_epochs = None\n",
    "batch_size = None\n",
    "lr = None\n",
    "reg_coeff = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "##      Define optimizer, loss and sampler      ##\n",
    "##################################################\n",
    "\n",
    "optimizer = None\n",
    "criterion = None\n",
    "train_sampler = None\n",
    "val_sampler = None\n",
    "test_sampler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "Fill in the below cell to train the model. Store each epoch loss, accuracy and f1-score. Use f1-score to choose best epoch.\n",
    "\n",
    "**Note1**: To do backpropagation you need to first call `backward` function of criterion with 1 as its argument to have gradient of loss w.r.t output of this module and then using model `backward` function with `criterion.grads['x']` argument.\n",
    "\n",
    "**Note2**: You can ignore regularization term in your total loss value and just use criterion, but you must consider that during updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "##      Train and Validation loop      ##\n",
    "#########################################\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "train_f1, val_f1 = [], []\n",
    "best_model = None\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Train Phase\n",
    "    total_loss = 0\n",
    "    N = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    with tqdm.tqdm(enumerate(train_sampler), total=len(train_sampler)) as pbar:\n",
    "        for i, (x, y) in pbar:\n",
    "            #################################\n",
    "            ##          Your Code          ##\n",
    "            #################################\n",
    "            \n",
    "            acc = None\n",
    "            f1 = None\n",
    "            pbar.set_description(f'Train {epoch} | Loss:{total_loss/N:.2e} | Acc: {acc:.2f}| F1: {f1:.2f}|')\n",
    "    \n",
    "    # save epoch metrics for train phase\n",
    "    train_losses.append(total_loss/N)\n",
    "    train_accs.append(acc)\n",
    "    train_f1.append(f1)\n",
    "    \n",
    "\n",
    "    # Validation Phase\n",
    "    total_loss = 0\n",
    "    N = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    with tqdm.tqdm(enumerate(val_sampler), total=len(val_sampler)) as pbar:\n",
    "        for i, (x, y) in pbar:\n",
    "            #################################\n",
    "            ##          Your Code          ##\n",
    "            #################################\n",
    "            \n",
    "            acc = None\n",
    "            f1 = None\n",
    "            pbar.set_description(f'Val   {epoch} | Loss:{total_loss/N:.2e} | Acc: {acc:.2f}| F1: {f1:.2f}|')\n",
    "    \n",
    "    # save epoch metrics for validation phase\n",
    "    val_losses.append(total_loss/N)\n",
    "    val_accs.append(acc)\n",
    "    val_f1.append(f1)\n",
    "    \n",
    "    \n",
    "    #################################\n",
    "    ##       update best model     ##\n",
    "    ##          Your Code          ##\n",
    "    #################################\n",
    "    \n",
    "    \n",
    "    print(f'----------------------------[Epoch{epoch+1} finished!]----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "##      Plot train and validation loss, accuracy and f1 graphs      ##\n",
    "######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##                  Test your best model                  ##\n",
    "##          Report loss, accuracy and f1 metrics          ##\n",
    "##      Also plot the confusion matrix for test data      ##\n",
    "############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Visualization (5 Points)\n",
    "\n",
    "For the last part we want to visualize weights matrix of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##          Visualize n of first layer weights          ##\n",
    "##          First reshape them to (32, 32, 3)           ##\n",
    "##########################################################\n",
    "n = 5\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "91b410dd9a8bb11c536d001ad40742c19c082a60dfe79daca26509b38e876541"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
